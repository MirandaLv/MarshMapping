{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83527e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "The data hub registration and usage: https://scihub.copernicus.eu/userguide/WebHome\n",
    "\n",
    "Main API currently only for downloading Sentinel 1/2/3 data\n",
    "https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/\n",
    "https://sentinel.esa.int/web/sentinel/technical-guides/sentinel-2-msi\n",
    "https://sentinel.esa.int/documents/247904/685211/Sentinel-2-Products-Specification-Document\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "from shapely.geometry import box, Polygon\n",
    "import numpy as np\n",
    "path_cur = os.path.abspath('.')\n",
    "from os.path import dirname as up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(os.path.join(up(path_cur), 'raw_data', 'sentinel'))\n",
    "\n",
    "(base_path / 'sentinel' / 'tmp').mkdir(exist_ok=True, parents=True)\n",
    "(base_path / 'sentinel' / 'compressed').mkdir(exist_ok=True, parents=True)\n",
    "(base_path / 'sentinel' / 'uncompressed').mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa4dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2016, 2017\n",
    "# 2018, 2019\n",
    "year = 2018\n",
    "temporal_start = '{}0501'.format(year)\n",
    "temporal_end = '{}0930'.format(year)\n",
    "\n",
    "platformname = 'Sentinel-2'\n",
    "processinglevel = 'Level-1C'\n",
    "\n",
    "max_cloud_percent = 10\n",
    "\n",
    "user = 'mirandalv12'\n",
    "password = '!u@EfpYBYmWL7W5'\n",
    "\n",
    "\n",
    "# api_url = 'https://scihub.copernicus.eu/dhus'\n",
    "api_url = 'https://apihub.copernicus.eu/apihub/'\n",
    "\n",
    "def get_poly_bound(gdf):\n",
    "    \n",
    "    bounds = gdf['geometry'].bounds\n",
    "    minx = np.min(bounds.minx)\n",
    "    miny = np.min(bounds.miny)\n",
    "    maxx = np.max(bounds.maxx)\n",
    "    maxy = np.max(bounds.maxy)\n",
    "    \n",
    "    return minx, miny, maxx, maxy\n",
    "\n",
    "\n",
    "boundary_file = os.path.join(up(path_cur), 'data', 'sir2005-5073_shed_shape', 'cbshed_boundary.geojson')\n",
    "# boundary_file = os.path.join(up(path_cur), 'data', 'TMI_2011_2019', 'VA_TMI_2011_2019.shp')\n",
    "\n",
    "boundary_df = gpd.read_file(boundary_file)\n",
    "boundary_df = boundary_df.to_crs(4326)\n",
    "bounds = boundary_df['geometry'].bounds\n",
    "\n",
    "minx, miny, maxx, maxy = get_poly_bound(boundary_df)\n",
    "new_poly = box(minx, miny, maxx, maxy)\n",
    "\n",
    "new_poly_geojson = gpd.GeoSeries([new_poly]).__geo_interface__\n",
    "footprint = geojson_to_wkt(new_poly_geojson)\n",
    "\n",
    "# # # search by polygon, time, and SciHub query keywords\n",
    "# footprint = geojson_to_wkt(read_geojson(boundary_file))\n",
    "\n",
    "\n",
    "api = SentinelAPI(user, password, api_url)\n",
    "products = api.query(footprint,\n",
    "                     date=(temporal_start, temporal_end),\n",
    "                     platformname=platformname,\n",
    "                     processinglevel=processinglevel,\n",
    "                     cloudcoverpercentage=(0, max_cloud_percent))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c274ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to Pandas DataFrame\n",
    "df = api.to_dataframe(products)\n",
    "\n",
    "\n",
    "df_path = base_path / 'sentinel_query_{}_{}.csv'.format(year, processinglevel)\n",
    "\n",
    "# save if running new query\n",
    "df.to_csv(df_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# load if query was already run\n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "# sort and limit to first 5 sorted products\n",
    "# df_sorted = df.sort_values(['cloudcoverpercentage', 'ingestiondate'], ascending=[True, True])\n",
    "# df_sorted = df_sorted.head(5)\n",
    "\n",
    "# api.download_all(df_sorted.uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d11d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"downloaded\"] = 0\n",
    "\n",
    "def set_download_granules(df, error_title=None):\n",
    "    if error_title is not None:\n",
    "        df.loc[df.title == error_title, \"downloaded\"] = -1\n",
    "    for i in set(df[\"tileid\"].to_list()):\n",
    "        best_title = df.loc[(df.tileid == i) & (df.downloaded != -1)].sort_values(\"cloudcoverpercentage\").iloc[0].title\n",
    "        df.loc[df.title == best_title, \"downloaded\"] = 1\n",
    "    return df\n",
    "\n",
    "\n",
    "def is_bad_zip(x):\n",
    "    \"\"\" Return true if zipfile is bad\n",
    "    \"\"\"\n",
    "    try:\n",
    "        test = zipfile.ZipFile(x).testzip()\n",
    "        if test is not None:\n",
    "            return test\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "# best_df = join_df.loc[join_df.groupby(\"Name\").cloudcoverpercentage.idxmin()]\n",
    "# best_df.cloudcoverpercentage.describe()\n",
    "# best_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df = set_download_granules(df, error_title=None)\n",
    "\n",
    "dl_base_path = base_path / 'sentinel'\n",
    "\n",
    "dl_tmp_path = os.path.join(dl_base_path, \"tmp\")\n",
    "dl_final_path = os.path.join(dl_base_path, \"compressed\")\n",
    "\n",
    "\n",
    "\n",
    "# remove bad downloads from tmp dir if needed\n",
    "for i in df.loc[df.downloaded == -1].title:\n",
    "    f = os.path.join(dl_base_path, \"tmp\", i + \".zip\")\n",
    "    if os.path.isfile(f):\n",
    "        print(i)\n",
    "        os.remove(f)\n",
    "\n",
    "\n",
    "# remove entire tmp dir and recreate\n",
    "shutil.rmtree(dl_tmp_path)\n",
    "os.makedirs(dl_tmp_path)\n",
    "\n",
    "\n",
    "# # remove downloads from compressed dir if no longer in current download list\n",
    "# current_contents = [os.path.splitext(i)[0] for i in os.listdir(dl_final_path)]\n",
    "# old_content = [i for i in current_contents if i not in df.loc[df.downloaded == 1].title.to_list()]\n",
    "\n",
    "# for i in old_content:\n",
    "#     print(\"Removing\", i)\n",
    "#     os.remove(os.path.join(dl_final_path, i + \".zip\"))\n",
    "\n",
    "\n",
    "# get rid of any bad zips from previous runs\n",
    "# (mainly for removing bad zips from before code checked zip immediately after download)\n",
    "best_df = df.loc[df.downloaded == 1]\n",
    "for i in range(len(best_df)):\n",
    "    base_name = best_df.title.iloc[i] + \".zip\"\n",
    "    final_path = os.path.join(dl_final_path, base_name)\n",
    "    if os.path.isfile(final_path) and is_bad_zip(final_path):\n",
    "        print(\"Removing\", best_df.title.iloc[i])\n",
    "        # os.remove(final_path)\n",
    "        # join_df = set_download_granules(join_df, error_title=best_df.title.iloc[i])\n",
    "\n",
    "\n",
    "# bad_titles = []\n",
    "\n",
    "# not_coming_online = []\n",
    "\n",
    "# for i in bad_titles + not_coming_online:\n",
    "#     df = set_download_granules(df, i)\n",
    "\n",
    "\n",
    "\n",
    "# to_download = []\n",
    "\n",
    "# current_contents = [os.path.splitext(i)[0] for i in os.listdir(dl_final_path)]\n",
    "\n",
    "# df.loc[df.title.isin(current_contents + to_download), \"downloaded\"] = 1\n",
    "# df.loc[df.title.isin(bad_titles + not_coming_online), \"downloaded\"] = -1\n",
    "\n",
    "\n",
    "# for i, row in df.loc[(df.downloaded == 1) & df.tileid.isin([\"51PWL\",\"51NYH\"])].iterrows():\n",
    "#     print(row.title, row.cloudcoverpercentage)\n",
    "\n",
    "# set_zero = []\n",
    "\n",
    "# df.loc[df.title.isin(set_zero), \"downloaded\"] = 0\n",
    "\n",
    "\n",
    "# df.loc[df.downloaded == 1]\n",
    "\n",
    "# df.loc[df.downloaded == 1].tileid.value_counts()\n",
    "\n",
    "# df.loc[df.downloaded == 1].cloudcoverpercentage.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabefcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b628fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to true for initial run where data may need to be retrieved from\n",
    "# the long term archive (lta). If you know data is available, set to\n",
    "# false to allow for faster downloads\n",
    "lta_query = True\n",
    "\n",
    "any_offline = True\n",
    "\n",
    "while any_offline:\n",
    "    print(\"Restarting download checks...\")\n",
    "    any_offline = False\n",
    "    best_df = df.loc[df.downloaded == 1]\n",
    "    existing_files = os.listdir(dl_final_path)\n",
    "    for i in range(len(best_df)):\n",
    "        base_name = best_df.title.iloc[i] + \".zip\"\n",
    "        tmp_path = os.path.join(dl_tmp_path, base_name)\n",
    "        final_path = os.path.join(dl_final_path, base_name)\n",
    "        # if is_bad_zip(final_path):\n",
    "        #     os.remove(final_path)\n",
    "        if base_name not in existing_files:\n",
    "        # if not os.path.isfile(final_path):\n",
    "            print(\"\\t{}/{} - {}\".format(i+1, len(best_df), best_df.title.iloc[i]))\n",
    "            try:\n",
    "                tmp = api.download(best_df.uuid.iloc[i], directory_path=dl_tmp_path)\n",
    "                if tmp[\"Online\"]:\n",
    "                    zip_error = is_bad_zip(tmp_path)\n",
    "                    if zip_error:\n",
    "                        print(\"\\t\\tbad zip:\", zip_error)\n",
    "                        df = set_download_granules(df, error_title=best_df.title.iloc[i])\n",
    "                        os.remove(tmp_path)\n",
    "                        any_offline = True\n",
    "                    else:\n",
    "                        shutil.move(tmp_path, final_path)\n",
    "            except Exception as e:\n",
    "                print(\"API Error:\", best_df.title.iloc[i], e)\n",
    "                # raise\n",
    "                print(best_df.title.iloc[i])\n",
    "                df = set_download_granules(df, error_title=best_df.title.iloc[i])\n",
    "                any_offline = True\n",
    "            else:\n",
    "                any_offline = True\n",
    "            if lta_query:\n",
    "                time.sleep(60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "granules_df_path = base_path / 'sentinel/sentinel_granules.csv'\n",
    "\n",
    "# save if running new query\n",
    "df.to_csv(granules_df_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "df = pd.read_csv(granules_df_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26600f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# other api functions:\n",
    "\n",
    "# download all results from the search\n",
    "api.download_all(df.uuid, directory_path=dl_path, n_concurrent_dl=3)\n",
    "\n",
    "# GeoJSON FeatureCollection containing footprints and metadata of the scenes\n",
    "api.to_geojson(products)\n",
    "\n",
    "# GeoPandas GeoDataFrame with the metadata of the scenes and the footprints as geometries\n",
    "api.to_geodataframe(products)\n",
    "\n",
    "# Get basic information about the product: its title, file size, MD5 sum, date, footprint and\n",
    "# its download url\n",
    "api.get_product_odata(\"<product_id>\")\n",
    "\n",
    "# Get the product's full metadata available on the server\n",
    "api.get_product_odata(\"<product_id>\", full=True)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f27c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "import glob\n",
    "import multiprocessing\n",
    "\n",
    "year = '2016'\n",
    "\n",
    "zip_list = glob.glob('/rapids/notebooks/sciclone/geograd/Miranda/github/MarshMapping/raw_data/sentinel/sentinel/compressed/*_*_{}*'.format(year))\n",
    "\n",
    "uncompressed_dir = base_path / 'sentinel/uncompressed'\n",
    "\n",
    "\n",
    "def unzip(zip_path, overwrite=False):\n",
    "    base_name = os.path.basename(zip_path).split('.')[0]\n",
    "    print(\"Unzipping\", base_name)\n",
    "    safe_name = base_name + \".SAFE\"\n",
    "    safe_dir = os.path.join(uncompressed_dir, safe_name)\n",
    "    exists = os.path.isdir(safe_dir)\n",
    "    if overwrite and exists:\n",
    "        print(\"\\tOverwritting\")\n",
    "        zipObj = ZipFile(zip_path, 'r')\n",
    "        zipObj.extractall(uncompressed_dir)\n",
    "    elif not exists:\n",
    "        print(\"\\tWritting\")\n",
    "        zipObj = ZipFile(zip_path, 'r')\n",
    "        zipObj.extractall(uncompressed_dir)\n",
    "    else:\n",
    "        print(\"\\tSkipping existing\")\n",
    "\n",
    "with multiprocessing.Pool(2) as p:\n",
    "    p.map(unzip, zip_list)\n",
    "\n",
    "for i in zip_list:\n",
    "    try:\n",
    "        x = ZipFile(i)\n",
    "    except:\n",
    "        print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944fcbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uncompressed_list = [os.path.splitext(os.path.basename(i))[0] for i in glob.glob('/rapids/notebooks/sciclone/geograd/Miranda/github/MarshMapping/raw_data/sentinel/sentinel/uncompressed/*_*_{}*'.format(year))]\n",
    "compressed_list = [os.path.splitext(os.path.basename(i))[0] for i in glob.glob('/rapids/notebooks/sciclone/geograd/Miranda/github/MarshMapping/raw_data/sentinel/sentinel/compressed/*_*_{}*'.format(year))]\n",
    "\n",
    "extra_uncompressed_list = [i for i in uncompressed_list if i not in compressed_list]\n",
    "missing_compressed_list = [i for i in compressed_list if i not in uncompressed_list]\n",
    "\n",
    "for i in extra_uncompressed_list:\n",
    "    extra_dir = base_path / 'sentinel/uncompressed/' + i + '.SAFE'\n",
    "    shutil.rmtree(extra_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d479a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_projection(intif, outtif):\n",
    "        \n",
    "    dst_crs = 'EPSG:4326'\n",
    "    \n",
    "    src = rasterio.open(intif, driver='JP2OpenJPEG')\n",
    "    transform, width, height = calculate_default_transform(src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "    kwargs = src.meta.copy()\n",
    "    kwargs.update({\n",
    "        'crs': dst_crs,\n",
    "        'transform': transform,\n",
    "        'width': width,\n",
    "        'height': height\n",
    "    })\n",
    "    \n",
    "    with rasterio.open(outtif, 'w', **kwargs) as dst:\n",
    "        for i in range(1, src.count + 1):\n",
    "            reproject(\n",
    "                source=rasterio.band(src, i),\n",
    "                destination=rasterio.band(dst, i),\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=transform,\n",
    "                dst_crs=dst_crs,\n",
    "                resampling=Resampling.nearest)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8886316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "\n",
    "import glob\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from osgeo import gdal\n",
    "\n",
    "b_path = \"/rapids/notebooks/sciclone/geograd/Miranda/github/MarshMapping/raw_data/sentinel\"\n",
    "\n",
    "(base_path / 'sentinel' / 'reprojection').mkdir(exist_ok=True, parents=True)\n",
    "reproject_path = os.path.join(b_path, 'sentinel','reprojection')\n",
    "\n",
    "band_list = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"8A\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "# for b in band_list:\n",
    "# b = \"03\"\n",
    "\n",
    "for b in band_list:\n",
    "    \n",
    "    print(\"Working on band {}\".format(b))\n",
    "    \n",
    "    band_granules_path = glob.glob(os.path.join(b_path, 'sentinel/uncompressed/*_*_{}*/GRANULE/*/IMG_DATA/*_B{}.jp2'.format(year, b)))\n",
    "\n",
    "    band_granules_src = [rasterio.open(i, driver='JP2OpenJPEG') for i in band_granules_path]\n",
    "    proj_band_path = [os.path.join(reproject_path, Path(i).stem + '_wgs84.tif') for i in band_granules_path]\n",
    "\n",
    "    # Reproject all bands to wgs-84 before mosaic\n",
    "    for file,outfile in zip(band_granules_path, proj_band_path):\n",
    "\n",
    "        infilename = os.path.basename(file).split('.')[0]\n",
    "        outfilename_pre = os.path.basename(outfile).split('.')[0][0:-6]\n",
    "\n",
    "        if infilename == outfilename_pre and not os.path.isfile(outfile):\n",
    "             \n",
    "             print(\"Reproject file {}\".format(file))\n",
    "             \n",
    "             re_projection(file, outfile)\n",
    "\n",
    "    print(\"proj_band_path is {}\".format(proj_band_path))\n",
    "    # Create mosaic band\n",
    "    \n",
    "    print(\"Start mosaic band {} in year {}\".format(b, year))\n",
    "    mosaic_path = base_path / 'sentinel/merge_B{}_{}.tif'.format(b, year)\n",
    "    \n",
    "    if not os.path.isfile(mosaic_path):\n",
    "\n",
    "        new_band_granules_src = [rasterio.open(i) for i in proj_band_path]\n",
    "\n",
    "        mosaic, output = merge(new_band_granules_src)\n",
    "\n",
    "        output_meta = new_band_granules_src[0].meta.copy()\n",
    "        output_meta.update(\n",
    "                {\"driver\": \"GTiff\",\n",
    "                    \"height\": mosaic.shape[1],\n",
    "                    \"width\": mosaic.shape[2],\n",
    "                    \"transform\": output,\n",
    "                }\n",
    "        )\n",
    "\n",
    "        with rasterio.open(mosaic_path, \"w\", **output_meta) as m:\n",
    "            m.write(mosaic)\n",
    "\n",
    "\n",
    "# for b in band_list:\n",
    "    \n",
    "#     band_granules_path = glob.glob(os.path.join(b_path, 'sentinel/uncompressed/*/GRANULE/*/IMG_DATA/*_B{}.jp2'.format(b)))\n",
    "\n",
    "#     band_granules_src = [rasterio.open(i, driver='JP2OpenJPEG') for i in band_granules_path]\n",
    "\n",
    "#     mosaic_path = base_path / 'sentinel/merge_B{}_{}.tif'.format(b, year)\n",
    "\n",
    "#     # # https://rasterio.readthedocs.io/en/latest/api/rasterio.merge.html\n",
    "#     # band_mosaic = merge(band_granules_src, method=\"first\", dst_path=mosaic_path, dst_kwds={\"Driver\": \"GeoTiff\"})\n",
    "\n",
    "\n",
    "#     mosaic, output = merge(band_granules_src)\n",
    "\n",
    "#     output_meta = band_granules_src[-1].meta.copy()\n",
    "#     output_meta.update(\n",
    "#         {\"driver\": \"GTiff\",\n",
    "#             \"height\": mosaic.shape[1],\n",
    "#             \"width\": mosaic.shape[2],\n",
    "#             \"transform\": output,\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     with rasterio.open(mosaic_path, \"w\", **output_meta) as m:\n",
    "#         m.write(mosaic)\n",
    "\n",
    "\n",
    "# # convert from utm51n to wgs84?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cdd2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vims",
   "language": "python",
   "name": "vims"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
